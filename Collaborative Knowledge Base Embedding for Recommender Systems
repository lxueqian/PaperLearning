2019 KDD
背景
	由于用户-物品交互的稀疏性，协同过滤方法效果有时不好，需要附加信息提升模型效果，这样的推荐系统叫做hybrid recommender
	知识库提供了不同语义的结构化和非结构化的异质信息，使用异质信息可以帮助信息检索、电信诈骗和语义分析
	利用知识库里面的异质信息来提升推荐效果，在hybrid recommender使用知识库很流行
本文做法（简单版）
	先从结构化、文本和视觉方面提取物品的语义表示（transR/stacked denoising auto-encoders/stacked convolutional auto-encoders分别提取这三方面的信息）
	CKE来联合学习协同过滤中的隐含表示和来自知识库的物品的语义表示
前人研究
	用异质信息网络来表示知识库中的用户、物品、物品属性和关系，提取基于mata_path的网络结构中的隐含特征，采用基于贝叶斯排序优化的协同过滤来解决实体推荐问题
	采用a spreading activation based的协同过滤？？来把知识库的网络特征结合到推荐中
	之前的文章不足：利用知识库不充分，一是只利用结构化信息忽视文本或者视觉等信息；二是需要从知识库中人工提取特征，特征工程很繁琐
本文的工作
	提出一个推荐框架来结合协同过滤和物品来自知识库的语义表示
	考虑知识库的结构化信息+文本信息+视觉信息（如海报）
	为了防止人工繁杂的特征工程，设计了三种embedding组件来自动从知识库的结构化、文本和视觉内容中提取物品的语义表示
	具体工作：
	先用一个network embedding的方法提取物品结构化表示（考虑节点和关系的异质性）
	再用SDAE和SCAE这两种基于深度学习的嵌入方法来提取物品的文本表示和视觉表示
	最后用Collaborative Knowledge Base Embedding的方法来整合协同过滤和物品的语义表示，联合学习
本文的贡献
	第一个综合考虑知识库中的结构化+文本+视觉信息来做推荐
	使用了异质网络嵌入方法和深度学习嵌入方法来自动从知识库提取物品的语义表示
	为了共同训练知识库嵌入和协同过滤，CKE可以同时从知识库提取特征表示和捕捉用户物品之间的隐含关系
本文的重要概念
	用户隐式反馈：本文针对隐式反馈，1不代表喜欢，0也不代表不喜欢
	知识库：
	知识库中的项目可以被映射到一个实体上，比如一个电影的一些特征可以被映射到一个描述这部电影的实体上（item entities）
	认为知识库中的知识可以分为以下三种：
		结构化知识：用不同的实体和关系组成的知识（属性也是一个实体？）
		文本知识：书或者电影的中心思想
		视觉知识：比如书的封面或者电影海报
	用户的隐式反馈+结构化知识=物品的结构化特征
	文本+视觉=内容特征

问题描述
         给出一个具有结构化知识、文本和视觉知识，以及用户隐式反馈，给每一个用户推荐一系列他可能喜欢的物品
方法综述
	知识库嵌入
	提取物品实体的三个嵌入向量（从结构化、文本、视觉三方面），分别含有该物品在这三个领域的隐含表示：1）结构化嵌入部分，使用一个网络嵌入程序（Bayesian TransR）去寻找结构化异质网络中的隐含表示；2）使用非监督深度学习模型(Bayesian SDAE)去寻找文本方面的隐含表示；3）使用一个非监督的深度学习模型(Bayesian SCAE)去找到视觉方面的隐含表示。
	协同过滤联合学习
	一个物品的隐含向量最终表示为以上三个向量以及一个隐含抵消向量的结合
	最终物品向量含有结构化内容+文本+视觉+用户历史记录的信息
	之后用协同过滤CKE来优化物品之间的成对排序，联合学习用户隐式向量和物品隐式向量
 
具体方法：
	知识库嵌入
•	结构化嵌入
	表示结构化知识—无向图
	TransR是一种对于异质网络很好的嵌入方法，与其他认为实体和关系的嵌入向量都在一个空间内的方法不同，TransR把实体和关系表示在两个语义空间中，并用一个基于关系的矩阵联系起来
	Bayesian TransR，用四元组进行训练，得到物品的表示向量
•	文本嵌入
	SDAE 用encoder得到的向量作为物品文本方面的表示向量
•	视觉嵌入
	SCAE 还原破损的输入图片，工作原理与上面类似，只是用了CNN
	协同训练学习
•	CKE框架
•	用user和item的隐含向量，使用BPR的方法成对训练
•	目标函数以及优化参数
	Note：知识库中的实体究竟是什么？

实验
	数据库：电影和书籍（MovieLens/IntentBooks）
	结构化知识：
	a two staged method【24】先把电影映射到知识库中的实体上（注意在IntentBOOKS数据集中书已经是一个实体了就不用匹配了）；然后提取实体的子图，包括实体，与实体相关的实体（一跳），相应的关系。对于电影实体，那些与之相关的实体包括题材、演员、作者、导演、语言、国家、生产日期、评分、提名、获奖；对于书籍实体，包括题材、作者、发布日期、丛书系列、语言、评分
	文本知识：
	word hashing procedure【12】提取电影或书籍简介
	视觉知识：
	书籍封面或者电影海报
 
	评价指标：
	MAP@K (mean average precision) [26] + Recall@K
	每次的评价指标：5轮的均值，每轮换训练集
	训练集+测试集+验证集调参
	参数设置
 
	学习过程
	先把CF+SK/CF+TK/CF+VK分别与他们的baseline做比较
	CF+SK
BPRMF 没有用结构化信息
BPRMF+TransE： 为什么要用TransR，用了结构化信息但是不同的嵌入方法
PRP: PageRank with Priors[17]
PER: Personalized Entity Recommendation [30]
LIBFM(S): 把物品的结构化特征作为一个原始特征输入到LIBFM中
结论：NETWORK EMBEDDING的方法比直接作为特征输入会更好
	TransR方法更好
	CF+TK
LIBFM(T):把物品的文本特征作为一个原始特征输入
CMF(T): Collective Matrix Factorization [21] 同时分解多个矩阵来结合不同的数据源
CTR: Collaborative Topic Regression [28] 结合协同过滤和主题模型（strong baseline）
结论：文本信息比结构化信息更弱
	直接分解词矩阵不能充分利用文本信息
              CTR是一个很强的BASELINE
	CF+VK
LIBFM(V)：把物品的视觉特征作为一个原始特征输入
CMF(V): 同时分解购买记录和像素矩阵来结合不同的数据源
BPRMF+SDAE(V) 评价卷积层是否有用
结论：视觉信息最弱
	卷基层是有用的
	深度学习方法对于处理视觉信息比较有效

再把CF+SK+TK+VK与CKE(ST), CKE(SV), CKE(TV)，LIBFM(STV)（同样的方法只是把三个嵌入向量当作特征使用），BPRMF+STV（分别训练三个嵌入向量和CF，不联合训练）做比较

结论：结合三种信息更好
	嵌入比特征输入更有效
	联合训练CF和三个嵌入向量更有效
